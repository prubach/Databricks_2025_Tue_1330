{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384077be-21a7-4379-a371-da10229d1289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6469eaf4-7a4d-4a16-9131-c806213cf295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93ecc71-3ba9-475f-a40f-20ce40e8c2ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pure Python implementation, spark is not used\n",
    "# bin/kafka-topics.sh --create --bootstrap-server sih-60.cent.uw.edu.pl:49092 --partitions 1 --topic bigdata-test-26\n",
    "# bin/kafka-console-producer.sh --broker-list sih-60.cent.uw.edu.pl:49092 --topic bigdata-test-26\n",
    "from kafka import KafkaConsumer\n",
    "import sys\n",
    "import time\n",
    "\n",
    "consumer = KafkaConsumer('bigdata-test-26', bootstrap_servers=['sih-60.cent.uw.edu.pl:49092'], group_id='my_consumer')\n",
    "\n",
    "for message in consumer:\n",
    "  print(message.value.decode('utf-8'))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235065b7-c2a2-4ee4-aa15-aa92cc8b704b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"sih-60.cent.uw.edu.pl:49092\") \\\n",
    "  .option(\"subscribe\", \"bigdata-test-26\") \\\n",
    "  .load() \\\n",
    "  .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "  \n",
    "#.option(\"value.deserializer\", ) \\\n",
    "#df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "display(df, checkpointLocation = \"/Workspace/Users/pawel.rubach@sgh.waw.pl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b607e6a-ae85-4d08-b053-589395fd6a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_schema = StructType([\n",
    "  StructField('movieId', IntegerType()),\n",
    "  StructField('title', StringType()),\n",
    "  StructField('genres', StringType())\n",
    "])\n",
    "\n",
    "file_location = \"/databricks-datasets/cs110x/ml-1m/data-001/movies.dat\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \"::\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_movies = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(movies_schema) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401596eb-8cc7-4e31-8795-74e897d79a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ./kafka-console-consumer.sh --bootstrap-server sih-60.cent.uw.edu.pl:49092 --topic bigdata-test-26\n",
    "#spark.conf.set(\"spark.sql.streaming.stopActiveRunOnRestart\", True) \n",
    "ds = df_movies \\\n",
    "  .selectExpr(\"CAST(movieId AS STRING)\", \"CAST(title AS STRING) as value\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"sih-60.cent.uw.edu.pl:49092\") \\\n",
    "  .option(\"topic\", \"bigdata-test-26\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2026-01-20 Kafka",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
